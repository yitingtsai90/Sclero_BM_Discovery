{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1e6032",
   "metadata": {},
   "source": [
    "### Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9d5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### PyTorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "### Custom functions\n",
    "def relu(x):\n",
    "    return torch.max(torch.zeros(1,1),x) # Assuming x is already in torch format\n",
    "\n",
    "def relu_numpy(x):\n",
    "    return np.max(np.zeros(1,1),x) # Assuming x is in numpy format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d85b44",
   "metadata": {},
   "source": [
    "## Deep Learning model with Discriminative Center Loss (DL + DCL)\n",
    "\n",
    "Coded for the specific case of a 2-layer network (one middle hidden layer and one output layer), for a 2-class (binary) problem. Can be generalized to any number of layers and any number of classes by adjusting the relevant hyperparameters as necessary.\n",
    "\n",
    "Feature extraction code can be found after the DCLDL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 1E-1 # Center loss lambda value\n",
    "\n",
    "num_neurons_array = np.array([32]) # Middle hidden layer\n",
    "m = num_neurons_array.shape[0] + 1\n",
    "\n",
    "num_epoch = 1000\n",
    "num_experiments = 100\n",
    "\n",
    "d = X.shape[1]\n",
    "\n",
    "X_train_all = X\n",
    "y_train_all = y\n",
    "\n",
    "X_ANN_train = torch.tensor(X_train_all.astype('float32'))\n",
    "\n",
    "C = np.unique(y_train_all).shape[0] # Number of classes\n",
    "\n",
    "y_ANN_train = np.eye(C)[y_train_all.astype(int)] # Convert categorical to one-hot-encoding\n",
    "y_ANN_train = torch.tensor(y_ANN_train.astype('float32'))\n",
    "\n",
    "## Network loss function w.r.t. parameters:\n",
    "# Define loss w.r.t. parameters:\n",
    "def weight_loss_criterion(model,input,target):\n",
    "    # Grab parameters\n",
    "    p_dict = {}\n",
    "    ii = 0\n",
    "    for p in model.parameters():\n",
    "        p_dict[ii] = p\n",
    "        ii += 1\n",
    "\n",
    "    # Grab parameter norms\n",
    "    n_params = len(p_dict)\n",
    "    NN = input.shape[0]\n",
    "\n",
    "    # Forward prop:\n",
    "    ZL = {}\n",
    "    ZL[0] = input\n",
    "    for ii in range(1,m+1):\n",
    "        this_W = p_dict[2*ii-2].T\n",
    "        this_b = p_dict[2*ii-1].reshape(-1,1).T\n",
    "        ones_matrix = torch.ones((NN,1))\n",
    "        ZL[ii] = relu( torch.mm(ZL[ii-1],this_W) + torch.mm(ones_matrix,this_b) )\n",
    "        \n",
    "    # Backprop:\n",
    "    temp_softmax = torch.nn.Softmax(dim=1)\n",
    "    Z_Class_0 = ZL[m][y_train_all==0]\n",
    "    Z_Class_1 = ZL[m][y_train_all==1]\n",
    "    \n",
    "    Z_Center_0 = torch.mean(ZL[m][y_train_all==0],axis=0)\n",
    "    Z_Center_1 = torch.mean(ZL[m][y_train_all==1],axis=0)\n",
    "    \n",
    "    Center_loss = lamb*0.5*( torch.norm((Z_Class_0 - Z_Center_0),2) + torch.norm((Z_Class_1 - Z_Center_1),2) )\n",
    "    \n",
    "    loss = torch.norm( ( target - temp_softmax(ZL[m]) ) , 2 ) + Center_loss\n",
    "    \n",
    "    return loss, ZL, p_dict\n",
    "\n",
    "top_Class0_Positive_dict = {}\n",
    "top_Class0_Negative_dict = {}\n",
    "top_Class1_Positive_dict = {}\n",
    "top_Class1_Negative_dict = {}\n",
    "\n",
    "W_Class0_Positive_dict = {}\n",
    "W_Class0_Negative_dict = {}\n",
    "W_Class1_Positive_dict = {}\n",
    "W_Class1_Negative_dict = {}\n",
    "\n",
    "### START OF RUNS ###\n",
    "for run_num in range(0,num_experiments):\n",
    "    \n",
    "    p_array = [] # Parameters\n",
    "    Z_array = [] # Hidden layer values\n",
    "    loss_array = [] # Total loss\n",
    "\n",
    "    last_epoch_loss = 1\n",
    "    time_start = time.time()\n",
    "    while last_epoch_loss > 0.1:\n",
    "\n",
    "        ## Initialize Neural Network Architecture\n",
    "        model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d,num_neurons_array[0], bias=True),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(num_neurons_array[0],C, bias=True),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay = 0.1)\n",
    "        loss_list = []\n",
    "\n",
    "        for epoch in range(0,num_epoch):\n",
    "\n",
    "            input = Variable(X_ANN_train)\n",
    "            target = Variable(y_ANN_train)\n",
    "\n",
    "            # Forward prop\n",
    "            out = model(input)\n",
    "            loss, ZL, p_dict = weight_loss_criterion(model,input,target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch == 0:\n",
    "                first_epoch_loss = loss.data.item()\n",
    "\n",
    "            this_loss = loss.data.item()\n",
    "            loss_list.append(loss.data.item())\n",
    "\n",
    "            ## Early-stopping criteria \n",
    "            earliest_stopping_epoch = 400\n",
    "\n",
    "            if epoch >= earliest_stopping_epoch and loss.data.item() > 1:\n",
    "                break\n",
    "\n",
    "        last_epoch_loss = loss.data.item()\n",
    "        print(\"Starting loss of {:.4f} and ending loss of {:.4f}, on epoch {}.\".format(first_epoch_loss,last_epoch_loss,epoch))\n",
    "\n",
    "    print(\"Run {} now finished, with starting loss of {:.4f} and ending loss of {:.4f}, on epoch {}.\".format(run_num,first_epoch_loss,last_epoch_loss,epoch))\n",
    "    time_end = time.time()\n",
    "    print(\"Total elapsed time: {:.1f} sec.\".format(time_end-time_start))\n",
    "\n",
    "    Z_array.append(ZL) # Hidden layer values\n",
    "    p_array.append(p_dict)\n",
    "\n",
    "    X_train_all_this = X_train_all\n",
    "\n",
    "    #### LATENT SPACE PLOTS ####\n",
    "\n",
    "    ### Grab optimal DL model parameters\n",
    "    ## Hidden Layer 1: k_1 = 100\n",
    "    W_1 = p_array[0][0].detach().numpy().T\n",
    "    b_1 = p_array[0][1].detach().numpy()\n",
    "\n",
    "    ## Hidden Layer 2 (Pre-Softmax): k_2 = C = 2\n",
    "    W_2 = p_array[0][2].detach().numpy().T\n",
    "    b_2 = p_array[0][3].detach().numpy()\n",
    "\n",
    "    ### Calculate all sample values in Hidden Layers 1 and 2:\n",
    "    B_1_train = np.ones((N_train,num_neurons_array[0]))\n",
    "    B_2_train = np.ones((N_train,C))\n",
    "\n",
    "    for i in range(0,N_train):\n",
    "        B_1_train[i,:] = b_1.reshape(1,-1)\n",
    "        B_2_train[i,:] = b_2.reshape(1,-1)\n",
    "\n",
    "    ## Training samples\n",
    "    Z_train_HL1 = np.matmul(X_train_all_this,W_1) + B_1_train\n",
    "    Z_train_HL1 = relu(torch.tensor(Z_train_HL1.astype('float32'))).detach().numpy()\n",
    "\n",
    "    Z_train_HL2 = np.matmul(Z_train_HL1,W_2) + B_2_train\n",
    "    Z_train_HL2 = relu(torch.tensor(Z_train_HL2.astype('float32'))).detach().numpy()\n",
    "\n",
    "    Z_train_HL2_0 = Z_train_HL2[y_train_all==0] # Class 0, ACA+\n",
    "    Z_train_HL2_1 = Z_train_HL2[y_train_all==1] # Class 1, SCL70+\n",
    "\n",
    "    ## Latent Space Plots\n",
    "    dim_1 = 0\n",
    "    dim_2 = 1\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.title(\"Run {}\".format(run_num))\n",
    "    plt.scatter(Z_train_HL2_0[:,dim_1], Z_train_HL2_0[:,dim_2], marker='o', s=60, color='blue', label = \"ACA+ (Class 0)\")\n",
    "    plt.scatter(Z_train_HL2_1[:,dim_1], Z_train_HL2_1[:,dim_2], marker='x', s=60, color='red', label = \"SCL70+ (Class 1)\")\n",
    "    plt.xlabel(\"$Z_1$\")\n",
    "    plt.ylabel(\"$Z_2$\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(2,1))\n",
    "    plt.show()\n",
    "\n",
    "    #### FEATURE EXTRACTION (ENSEMBLE APPROACH) ####\n",
    "    #### Second hidden layer weight traceback\n",
    "    W_HL2 = p_array[0][2].T.detach().numpy()\n",
    "    W_HL2_0 = W_HL2[:,0]\n",
    "    W_HL2_1 = W_HL2[:,1]\n",
    "\n",
    "    print(\"Hidden Layer 2 -- Class 0 -- Total number of Positive Weights: {}\".format(np.sum(W_HL2_0 > 0)))\n",
    "    print(\"Hidden Layer 2 -- Class 0 -- Total number of Negative Weights: {}\".format(np.sum(W_HL2_0 < 0)))\n",
    "    print(\"Hidden Layer 2 -- Class 1 -- Total number of Positive Weights: {}\".format(np.sum(W_HL2_1 > 0)))\n",
    "    print(\"Hidden Layer 2 -- Class 1 -- Total number of Negative Weights: {}\".format(np.sum(W_HL2_1 < 0)))\n",
    "    print(\"\")\n",
    "\n",
    "    m_top_HL2 = 4 # Number of top weights to consider during traceback\n",
    "\n",
    "    ## Find most positively-correlated weights\n",
    "    # Z_0 - Pertaining to Class 0\n",
    "    top_HL2_Class0_Positive = (-W_HL2_0).argsort()[0:m_top_HL2]\n",
    "    print(\"Hidden Layer 2 -- Class 0 Most Positive Weights: {}\".format(W_HL2_0[top_HL2_Class0_Positive]))\n",
    "    print(\"\")\n",
    "\n",
    "    # Z_1 - Pertaining to Class 1\n",
    "    top_HL2_Class1_Positive = (-W_HL2_1).argsort()[0:m_top_HL2]\n",
    "    print(\"Hidden Layer 2 -- Class 1 Most Positive Weights: {}\".format(W_HL2_1[top_HL2_Class1_Positive]))\n",
    "    print(\"\")\n",
    "\n",
    "    ## Find most negatively-correlated weights\n",
    "    # Z_0 - Pertaining to Class 0\n",
    "    top_HL2_Class0_Negative = (W_HL2_0).argsort()[0:m_top_HL2]\n",
    "    print(\"Hidden Layer 2 -- Class 0 Most Negative Weights: {}\".format(W_HL2_0[top_HL2_Class0_Negative]))\n",
    "    print(\"\")\n",
    "\n",
    "    # Z_1 - Pertaining to Class 1\n",
    "    top_HL2_Class1_Negative = (W_HL2_1).argsort()[0:m_top_HL2]\n",
    "    print(\"Hidden Layer 2 -- Class 1 Most Negative Weights: {}\".format(W_HL2_1[top_HL2_Class1_Negative]))\n",
    "    print(\"\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    #### First hidden layer weight traceback\n",
    "    W_HL1 = p_array[0][0].T.detach().numpy()\n",
    "    m_top_HL1 = 10 # Number of top weights to consider during traceback\n",
    "\n",
    "    # Weights most positively-correlated to Class 0\n",
    "    top_Class0_Positive_pospos = (-W_HL1[:,top_HL2_Class0_Positive]).argsort(axis=0)[0:m_top_HL1]\n",
    "    top_Class0_Positive_negneg = (W_HL1[:,top_HL2_Class0_Negative]).argsort(axis=0)[0:m_top_HL1]\n",
    "    top_Class0_Positive = np.concatenate((top_Class0_Positive_pospos,top_Class0_Positive_negneg),axis=1)\n",
    "\n",
    "    # Weights most positively-correlated to Class 1\n",
    "    top_Class1_Positive_pospos = (-W_HL1[:,top_HL2_Class1_Positive]).argsort(axis=0)[0:m_top_HL1]\n",
    "    top_Class1_Positive_negneg = (W_HL1[:,top_HL2_Class1_Negative]).argsort(axis=0)[0:m_top_HL1]\n",
    "    top_Class1_Positive = np.concatenate((top_Class1_Positive_pospos,top_Class1_Positive_negneg),axis=1)\n",
    "\n",
    "    # Weights most negatively-correlated to Class 0\n",
    "    top_Class0_Negative_posneg = (-W_HL1[:,top_HL2_Class0_Negative]).argsort(axis=0)[0:m_top_HL1]\n",
    "    top_Class0_Negative_negpos = (W_HL1[:,top_HL2_Class0_Positive]).argsort(axis=0)[0:m_top_HL1]\n",
    "    top_Class0_Negative = np.concatenate((top_Class0_Negative_posneg,top_Class0_Negative_negpos),axis=1)\n",
    "\n",
    "    # Weights most negatively-correlated to Class 1\n",
    "    top_Class1_Negative_posneg = (-W_HL1[:,top_HL2_Class1_Negative]).argsort(axis=0)[0:m_top_HL1]\n",
    "    top_Class1_Negative_negpos = (W_HL1[:,top_HL2_Class1_Positive]).argsort(axis=0)[0:m_top_HL1]\n",
    "    top_Class1_Negative = np.concatenate((top_Class1_Negative_posneg,top_Class1_Negative_negpos),axis=1)\n",
    "\n",
    "    # Compile results\n",
    "    W_Class0_Positive = np.zeros((m_top_HL1,m_top_HL2*2))\n",
    "    W_Class1_Positive = np.zeros((m_top_HL1,m_top_HL2*2))\n",
    "    W_Class0_Negative = np.zeros((m_top_HL1,m_top_HL2*2))\n",
    "    W_Class1_Negative = np.zeros((m_top_HL1,m_top_HL2*2))\n",
    "\n",
    "    for j in range(0,m_top_HL2):\n",
    "        W_Class0_Positive[:,j] = W_HL1[:,top_HL2_Class0_Positive][:,j][top_Class0_Positive_pospos[:,j]]\n",
    "        W_Class0_Positive[:,j+m_top_HL2] = W_HL1[:,top_HL2_Class0_Negative][:,j][top_Class0_Positive_negneg[:,j]]\n",
    "\n",
    "        W_Class1_Positive[:,j] = W_HL1[:,top_HL2_Class1_Positive][:,j][top_Class1_Positive_pospos[:,j]]\n",
    "        W_Class1_Positive[:,j+m_top_HL2] = W_HL1[:,top_HL2_Class1_Negative][:,j][top_Class1_Positive_negneg[:,j]]\n",
    "\n",
    "        W_Class0_Negative[:,j] = W_HL1[:,top_HL2_Class0_Negative][:,j][top_Class0_Negative_posneg[:,j]]\n",
    "        W_Class0_Negative[:,j+m_top_HL2] = W_HL1[:,top_HL2_Class0_Positive][:,j][top_Class0_Negative_negpos[:,j]]\n",
    "\n",
    "        W_Class1_Negative[:,j] = W_HL1[:,top_HL2_Class1_Negative][:,j][top_Class1_Negative_posneg[:,j]]\n",
    "        W_Class1_Negative[:,j+m_top_HL2] = W_HL1[:,top_HL2_Class1_Positive][:,j][top_Class1_Negative_negpos[:,j]]\n",
    "\n",
    "    top_Class0_Positive_dict[run_num] = top_Class0_Positive\n",
    "    top_Class0_Negative_dict[run_num] = top_Class0_Negative\n",
    "    top_Class1_Positive_dict[run_num] = top_Class1_Positive\n",
    "    top_Class1_Negative_dict[run_num] = top_Class1_Negative\n",
    "\n",
    "    W_Class0_Positive_dict[run_num] = W_Class0_Positive\n",
    "    W_Class0_Negative_dict[run_num] = W_Class0_Negative\n",
    "    W_Class1_Positive_dict[run_num] = W_Class1_Positive\n",
    "    W_Class1_Negative_dict[run_num] = W_Class1_Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Consolidate results ####\n",
    "def ensemblefy_DLFE(indices_dict,importances_dict,m_top_HL1,m_top_HL2):\n",
    "\n",
    "    final_DLFE_importances = np.zeros((m_top_HL1,2*m_top_HL2))\n",
    "    final_DLFE_indices = np.zeros((m_top_HL1,2*m_top_HL2))\n",
    "\n",
    "    for j in range(0,2*m_top_HL2):\n",
    "        these_DLFE_importances = []\n",
    "        these_DLFE_indices = []\n",
    "\n",
    "        for i in range(0,num_experiments):\n",
    "            these_DLFE_indices.append(indices_dict[i][:,j])\n",
    "            these_DLFE_importances.append(importances_dict[i][:,j])\n",
    "\n",
    "        these_DLFE_importances = np.abs(np.array(these_DLFE_importances))\n",
    "        these_DLFE_indices = np.array(these_DLFE_indices)\n",
    "\n",
    "        unique_DLFE_indices = np.unique(these_DLFE_indices)\n",
    "        unique_DLFE_importances = np.zeros((unique_DLFE_indices.shape[0],))\n",
    "\n",
    "        for ii in range(0,unique_DLFE_indices.shape[0]):\n",
    "            unique_DLFE_importances[ii] = np.sum(these_DLFE_importances[these_DLFE_indices == unique_DLFE_indices[ii]])\n",
    "\n",
    "        final_DLFE_importances[:,j] = -np.sort(-unique_DLFE_importances)[0:m_top_HL1]\n",
    "        eee = unique_DLFE_indices[np.argsort(-unique_DLFE_importances)[0:m_top_HL1]]\n",
    "        final_DLFE_indices[:,j] = eee.astype(int)\n",
    "\n",
    "    return final_DLFE_indices, final_DLFE_importances\n",
    "\n",
    "## Class 0 Positively-correlated\n",
    "[Class0_Positive_Final_Indices, Class0_Positive_Final_Importances] = ensemblefy_DLFE(top_Class0_Positive_dict,W_Class0_Positive_dict,m_top_HL1,m_top_HL2)\n",
    "Class0_Positive_Final_Importances = np.abs(Class0_Positive_Final_Importances)\n",
    "Class0_Positive_Final_Indices = Class0_Positive_Final_Indices.astype(int)\n",
    "\n",
    "## Class 0 Negative-correlated\n",
    "[Class0_Negative_Final_Indices, Class0_Negative_Final_Importances] = ensemblefy_DLFE(top_Class0_Negative_dict,W_Class0_Negative_dict,m_top_HL1,m_top_HL2)\n",
    "Class0_Negative_Final_Importances = -np.abs(Class0_Negative_Final_Importances)\n",
    "Class0_Negative_Final_Indices = Class0_Negative_Final_Indices.astype(int)\n",
    "\n",
    "## Class 1 Positively-correlated\n",
    "[Class1_Positive_Final_Indices, Class1_Positive_Final_Importances] = ensemblefy_DLFE(top_Class1_Positive_dict,W_Class1_Positive_dict,m_top_HL1,m_top_HL2)\n",
    "Class1_Positive_Final_Importances = np.abs(Class1_Positive_Final_Importances)\n",
    "Class1_Positive_Final_Indices = Class1_Positive_Final_Indices.astype(int)\n",
    "\n",
    "## Class 1 Negative-correlated\n",
    "[Class1_Negative_Final_Indices, Class1_Negative_Final_Importances] = ensemblefy_DLFE(top_Class1_Negative_dict,W_Class1_Negative_dict,m_top_HL1,m_top_HL2)\n",
    "Class1_Negative_Final_Importances = -np.abs(Class1_Negative_Final_Importances)\n",
    "Class1_Negative_Final_Indices = Class1_Negative_Final_Indices.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba43c5",
   "metadata": {},
   "source": [
    "## Deep Learning model without Discriminative Center Loss (DL only)\n",
    "\n",
    "Coded for the same specific case above, without DCL component or final feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_scaled\n",
    "y = y_labels\n",
    "\n",
    "\n",
    "num_neurons_array = np.array([32])\n",
    "m = num_neurons_array.shape[0] + 1\n",
    "\n",
    "num_epoch = 1000\n",
    "num_experiments = 100\n",
    "\n",
    "d = X.shape[1]\n",
    "\n",
    "X_train_all = X\n",
    "y_train_all = y\n",
    "\n",
    "X_ANN_train = torch.tensor(X_train_all.astype('float32'))\n",
    "\n",
    "C = np.unique(y_train_all).shape[0] # Number of classes\n",
    "\n",
    "y_ANN_train = np.eye(C)[y_train_all.astype(int)] # Convert categorical to one-hot-encoding\n",
    "y_ANN_train = torch.tensor(y_ANN_train.astype('float32'))\n",
    "\n",
    "FE_train_acc_array = []\n",
    "FE_test_acc_array = []\n",
    "\n",
    "y_train_dict = {}\n",
    "y_train_hat_dict = {}\n",
    "y_test_hat_dict = {}\n",
    "\n",
    "p_array = [] # Parameters\n",
    "Z_array = [] # Hidden layer values\n",
    "loss_array = [] # Total loss\n",
    "\n",
    "## Neural Network Architecture\n",
    "model = torch.nn.Sequential(\n",
    "torch.nn.Linear(d,num_neurons_array[0], bias=True),\n",
    "torch.nn.ReLU(),\n",
    "torch.nn.Linear(num_neurons_array[0],C, bias=True),\n",
    "torch.nn.ReLU(),\n",
    "torch.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "## Network loss function w.r.t. parameters:\n",
    "# Define loss w.r.t. parameters:\n",
    "def parameter_grab(model,input,target):\n",
    "    # Grab parameters\n",
    "    p_dict = {}\n",
    "    ii = 0\n",
    "    for p in model.parameters():\n",
    "        p_dict[ii] = p\n",
    "        ii += 1\n",
    "\n",
    "    # Grab parameter norms\n",
    "    n_params = len(p_dict)\n",
    "    p_norm_dict = {}\n",
    "    for ii in range(0,n_params):\n",
    "        p_norm_dict[ii] = torch.norm(p_dict[ii],2)\n",
    "\n",
    "    NN = input.shape[0]\n",
    "\n",
    "    # Forward prop:\n",
    "    ZL = {}\n",
    "    ZL[0] = input\n",
    "    for ii in range(1,m+1):\n",
    "        this_W = p_dict[2*ii-2].T\n",
    "        this_b = p_dict[2*ii-1].reshape(-1,1).T\n",
    "        ones_matrix = torch.ones((NN,1))\n",
    "        ZL[ii] = relu( torch.mm(ZL[ii-1],this_W) + torch.mm(ones_matrix,this_b) )\n",
    "\n",
    "    return ZL, p_dict, p_norm_dict\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay = 0.01)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(0,num_epoch):\n",
    "            \n",
    "    input = Variable(X_ANN_train)\n",
    "    target = Variable(y_ANN_train)\n",
    "\n",
    "    # Forward prop\n",
    "    out = model(input)\n",
    "    loss = loss_function(out, target)\n",
    "    ZL, p_dict, p_norm_dict = parameter_grab(model,input,target)\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print results per epoch\n",
    "    print('Epoch[{}/{}], loss: {:.6f}'\n",
    "          .format(epoch + 1, num_epoch, loss.data.item()))\n",
    "    this_loss = loss.data.item()\n",
    "    loss_list.append(loss.data.item())\n",
    "\n",
    "Z_array.append(ZL) # Hidden layer values\n",
    "print(\"This randomly-initialized ANN has finished training, with a final loss of {:.3f}.\".format(this_loss))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756f3ac",
   "metadata": {},
   "source": [
    "### Model accuracy evaluation (overall and class-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d38595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EVALUATE OVERALL MODEL ACCURACIES ####\n",
    "y_ANN_train_hat_np = model(X_ANN_train).data.numpy()\n",
    "y_ANN_train_hat = np.argmax(y_ANN_train_hat_np,axis=1)\n",
    "ANN_train_acc = np.sum(y_ANN_train_hat == y_train_all)/y_ANN_train_hat.shape[0]*100\n",
    "print(\"Overall Training Accuracy: {:.1f}%\".format(ANN_train_acc))\n",
    "\n",
    "#### EVALUATE INDIVIDUAL ACCURACIES ####\n",
    "for c in range(0,C):\n",
    "    y_class_train_hat = np.argmax(model(X_ANN_train[y_train_all == c]).data.numpy(),axis=1)\n",
    "    y_class_train = y_train_all[y_train_all == c]\n",
    "    class_train_acc = np.sum(y_class_train_hat == y_class_train)/y_class_train_hat.shape[0]*100\n",
    "    print(\"Class {} Testing Accuracy: {:.1f}%\".format(c,class_train_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
